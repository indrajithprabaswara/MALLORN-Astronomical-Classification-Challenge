{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDE Classification with Lightcurve-Derived 1-NN\n",
    "This notebook builds a lightweight nearest-neighbour classifier using summary statistics extracted from each lightcurve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('Data')\n",
    "FILTERS = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "FEATURE_LEN = 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(path):\n",
    "    with path.open(newline='') as handle:\n",
    "        return list(csv.DictReader(handle))\n",
    "\n",
    "def load_split_lightcurves(meta_rows, filename):\n",
    "    grouped = defaultdict(lambda: defaultdict(list))\n",
    "    for row in meta_rows:\n",
    "        grouped[row['split']][row['object_id']] = []\n",
    "    for split, objects in grouped.items():\n",
    "        with (DATA_DIR / split / filename).open(newline='') as handle:\n",
    "            reader = csv.DictReader(handle)\n",
    "            for entry in reader:\n",
    "                if entry['object_id'] in objects:\n",
    "                    objects[entry['object_id']].append(entry)\n",
    "    return grouped\n",
    "\n",
    "def lightcurve_features(meta_rows, split_data):\n",
    "    features = {}\n",
    "    for row in meta_rows:\n",
    "        object_id = row['object_id']\n",
    "        lc = split_data[row['split']][object_id]\n",
    "        fluxes = [float(entry['Flux']) for entry in lc if entry['Flux']]\n",
    "        times = [float(entry['Time (MJD)']) for entry in lc if entry['Flux']]\n",
    "        errs = [float(entry['Flux_err']) for entry in lc if entry['Flux_err']]\n",
    "        n = len(fluxes)\n",
    "        if n == 0:\n",
    "            vec = [0.0] * FEATURE_LEN\n",
    "        else:\n",
    "            mean_flux = sum(fluxes) / n\n",
    "            max_flux = max(fluxes)\n",
    "            min_flux = min(fluxes)\n",
    "            range_flux = max_flux - min_flux\n",
    "            pos_frac = sum(1 for v in fluxes if v > 0) / n\n",
    "            neg_frac = sum(1 for v in fluxes if v < 0) / n\n",
    "            mean_err = sum(errs) / len(errs) if errs else 0.0\n",
    "            mean_time = sum(times) / n if n else 0.0\n",
    "            max_time = times[fluxes.index(max_flux)] if n else 0.0\n",
    "            min_time = times[fluxes.index(min_flux)] if n else 0.0\n",
    "            per_filter = []\n",
    "            for flt in FILTERS:\n",
    "                flt_fluxes = [float(entry['Flux']) for entry in lc if entry['Filter'] == flt and entry['Flux']]\n",
    "                count = len(flt_fluxes)\n",
    "                if count:\n",
    "                    flt_mean = sum(flt_fluxes) / count\n",
    "                    flt_max = max(flt_fluxes)\n",
    "                    flt_min = min(flt_fluxes)\n",
    "                else:\n",
    "                    flt_mean = flt_max = flt_min = 0.0\n",
    "                per_filter.extend([count, flt_mean, flt_max, flt_min])\n",
    "            vec = [\n",
    "                n, mean_flux, max_flux, min_flux, range_flux, pos_frac, neg_frac, mean_err,\n",
    "                mean_time, max_time, min_time,\n",
    "                float(row['Z']) if row['Z'] else 0.0,\n",
    "                float(row['EBV']) if row['EBV'] else 0.0\n",
    "            ] + per_filter\n",
    "        features[object_id] = vec\n",
    "    return features\n",
    "\n",
    "def standardize(vectors):\n",
    "    length = len(next(iter(vectors.values())))\n",
    "    means = [0.0] * length\n",
    "    for vec in vectors.values():\n",
    "        for idx, val in enumerate(vec):\n",
    "            means[idx] += val\n",
    "    n = len(vectors)\n",
    "    means = [val / n for val in means]\n",
    "    stds = [0.0] * length\n",
    "    for vec in vectors.values():\n",
    "        for idx, val in enumerate(vec):\n",
    "            diff = val - means[idx]\n",
    "            stds[idx] += diff * diff\n",
    "    stds = [math.sqrt(val / n) if val > 0 else 1.0 for val in stds]\n",
    "    scaled = {obj: [(val - means[idx]) / stds[idx] for idx, val in enumerate(vec)]\n",
    "              for obj, vec in vectors.items()}\n",
    "    return scaled, means, stds\n",
    "\n",
    "def knn_predict(test_vectors, train_vectors, train_labels):\n",
    "    items = list(train_vectors.items())\n",
    "    predictions = {}\n",
    "    for object_id, vec in test_vectors.items():\n",
    "        best_dist = float('inf')\n",
    "        best_label = 0\n",
    "        for train_id, train_vec in items:\n",
    "            dist = math.dist(vec, train_vec)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_label = train_labels[train_id]\n",
    "                if dist == 0:\n",
    "                    break\n",
    "        predictions[object_id] = best_label\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = load_metadata(DATA_DIR / 'train_log.csv')\n",
    "test_meta = load_metadata(DATA_DIR / 'test_log.csv')\n",
    "train_lookup = {row['object_id']: row for row in train_meta}\n",
    "\n",
    "train_lightcurves = load_split_lightcurves(train_meta, 'train_full_lightcurves.csv')\n",
    "test_lightcurves = load_split_lightcurves(test_meta, 'test_full_lightcurves.csv')\n",
    "\n",
    "train_features = lightcurve_features(train_meta, train_lightcurves)\n",
    "train_vectors, feature_means, feature_stds = standardize(train_features)\n",
    "train_labels = {obj: int(train_lookup[obj]['target']) for obj in train_vectors}\n",
    "\n",
    "test_features = lightcurve_features(test_meta, test_lightcurves)\n",
    "test_vectors = {obj: [(val - feature_means[idx]) / feature_stds[idx]\n",
    "                     for idx, val in enumerate(vec)]\n",
    "                for obj, vec in test_features.items()}\n",
    "\n",
    "len(train_vectors), len(test_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-one-out estimate for the 1-NN model\n",
    "correct = 0\n",
    "train_items = list(train_vectors.items())\n",
    "for object_id, vec in train_items:\n",
    "    best_dist = float('inf')\n",
    "    best_label = 0\n",
    "    for other_id, other_vec in train_items:\n",
    "        if other_id == object_id:\n",
    "            continue\n",
    "        dist = math.dist(vec, other_vec)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_label = train_labels[other_id]\n",
    "            if dist == 0:\n",
    "                break\n",
    "    if best_label == train_labels[object_id]:\n",
    "        correct += 1\n",
    "loo_accuracy = correct / len(train_items)\n",
    "loo_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_predict(test_vectors, train_vectors, train_labels)\n",
    "submission_path = Path('submission.csv')\n",
    "with submission_path.open('w', newline='') as handle:\n",
    "    writer = csv.writer(handle)\n",
    "    writer.writerow(['object_id', 'prediction'])\n",
    "    for row in test_meta:\n",
    "        writer.writerow([row['object_id'], predictions[row['object_id']]])\n",
    "submission_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
